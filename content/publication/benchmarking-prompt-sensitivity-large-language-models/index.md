---
title: Benchmarking prompt sensitivity in large language models
authors:
- Amirhossein Razavi
- Mina Soltangheis
- Negar Arabzadeh
- Sara Salamat
- Morteza Zihayat
- Ebrahim Bagheri
date: '2025-01-01T00:00:00Z'
doi: ''
publishDate: '2025-01-01T00:00:00Z'
publication_types:
- paper-conference
publication: European Conference on Information Retrieval
publication_short: ECIR
abstract: Large language Models (LLMs) are highly sensitive to variations in prompt
  formulation, which can significantly impact their ability to generate accurate responses.
  In this paper, we introduce a new task, Prompt Sensitivity Prediction, and a dataset
  PromptSET designed to investigate the effects of slight prompt variations on LLM
  performance. Using TriviaQA and HotpotQA datasets as the foundation of our work,
  we generate prompt variations and evaluate their effectiveness across multiple LLMs.
  We benchmark the prompt sensitivity prediction task employing state-of-the-art methods
  from related tasks, including LLM-based self-evaluation, text classification, and
  query performance prediction techniques. Our findings reveal that existing methods
  struggle to effectively address prompt sensitivity prediction, underscoring the
  need to understand how information needs should be phrased for accurate LLM responses.
summary: Benchmarking prompt sensitivity in large language models.
tags:
- Large Language Models
- Prompt Sensitivity
- Benchmarking
- Model Robustness
- Information Retrieval
featured: false
url_pdf: ''
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''
image:
  caption: ''
  focal_point: ''
  preview_only: false
projects: []
slides: ''
---

This work benchmarks prompt sensitivity in large language models for improved understanding.
